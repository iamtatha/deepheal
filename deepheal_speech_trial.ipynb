{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0038bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import whisper\n",
    "import wavio\n",
    "import time\n",
    "import scipy.io.wavfile as wav\n",
    "from vosk import Model, KaldiRecognizer\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71554d4a",
   "metadata": {},
   "source": [
    "AUDIO TO TEXT TRASNCRIPTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53faf857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_audio(filename=\"output.wav\", duration=5, samplerate=16000):\n",
    "    \"\"\"Record audio from mic and save to a file\"\"\"\n",
    "    print(f\"üéôÔ∏è Recording for {duration} seconds...\")\n",
    "    recording = sd.rec(int(duration * samplerate), samplerate=samplerate, channels=1, dtype=np.int16)\n",
    "    sd.wait()  # Wait until recording finishes\n",
    "    wavio.write(filename, recording, samplerate, sampwidth=2)\n",
    "    print(f\"‚úÖ Saved audio as {filename}\")\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cbe6633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéôÔ∏è Recording for 5 seconds...\n",
      "‚úÖ Saved audio as output.wav\n"
     ]
    }
   ],
   "source": [
    "audio_file = record_audio(duration=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51868577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(filename):\n",
    "    \"\"\"Transcribe audio using Whisper\"\"\"\n",
    "    print(\"üìù Transcribing...\")\n",
    "    result = model.transcribe(filename)\n",
    "    text = result[\"text\"].strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9bbff47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output.wav\n",
      "üìù Transcribing...\n"
     ]
    }
   ],
   "source": [
    "model = whisper.load_model(\"base\")\n",
    "audio_file = \"output.wav\"\n",
    "print(audio_file)\n",
    "text = transcribe_audio(audio_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3508879e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It's not a big deal that I am trying to do this for ourselves.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d955bc33",
   "metadata": {},
   "source": [
    "AUDIO EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cbcaeaf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tathagata\\AppData\\Local\\Temp\\ipykernel_24284\\2029516171.py:6: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2Model, Wav2Vec2Processor\n",
    "import soundfile as sf\n",
    "\n",
    "torchaudio.set_audio_backend(\"soundfile\")\n",
    "\n",
    "\n",
    "def load_audio(path):\n",
    "    try:\n",
    "        waveform, sr = torchaudio.load(path)\n",
    "    except RuntimeError:\n",
    "        # fallback to soundfile\n",
    "        data, sr = sf.read(path, dtype=\"float32\")\n",
    "        waveform = torch.tensor(data).unsqueeze(0) if data.ndim == 1 else torch.tensor(data).T\n",
    "    return waveform, sr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826954ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 1 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<?, ?it/s]\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load model + processor\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31f62fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_embedding(audio_file):\n",
    "    waveform, sr = load_audio(audio_file)\n",
    "    inputs = processor(waveform, sampling_rate=sr, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Mean pooling over time\n",
    "    embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6237e399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([80000]) 16000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 1 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<?, ?it/s]\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embedding shape: (768,)\n",
      "First 10 dims: [-0.01119173  0.01588007 -0.08928814 -0.07473806  0.05007917 -0.08230261\n",
      "  0.05800665 -0.0366033   0.15863574 -0.22816195]\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "import soundfile as sf\n",
    "import torch\n",
    "\n",
    "# load audio\n",
    "def load_audio(path):\n",
    "    data, sr = sf.read(path, dtype=\"float32\")\n",
    "    if data.ndim > 1:  # stereo -> mono\n",
    "        data = data.mean(axis=1)\n",
    "    waveform = torch.tensor(data)  # [time]\n",
    "    return waveform, sr\n",
    "\n",
    "waveform, sr = load_audio(\"output.wav\")\n",
    "print(waveform.shape, sr) \n",
    "\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "# prepare inputs\n",
    "inputs = processor(waveform, sampling_rate=sr, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# mean-pool over time for fixed-length embedding\n",
    "embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "print(\"‚úÖ Embedding shape:\", embedding.shape)\n",
    "print(\"First 10 dims:\", embedding[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ee26d0",
   "metadata": {},
   "source": [
    "ACOUSTIC FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c6adc37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Pitch (Hz): 146.02637684880412\n",
      "Mean Loudness (RMS): 0.09505295\n",
      "MFCC shape: (13, 157)\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "y, sr = librosa.load(\"output.wav\", sr=None)\n",
    "\n",
    "# Pitch (tone) using librosa.yin\n",
    "pitch = librosa.yin(y, fmin=50, fmax=300)\n",
    "mean_pitch = np.mean(pitch)\n",
    "\n",
    "# Intensity (RMS Energy)\n",
    "rms = librosa.feature.rms(y=y)[0]\n",
    "mean_rms = np.mean(rms)\n",
    "\n",
    "# MFCCs (timbre)\n",
    "mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "mfcc_means = np.mean(mfccs, axis=1)\n",
    "\n",
    "print(\"Mean Pitch (Hz):\", mean_pitch)\n",
    "print(\"Mean Loudness (RMS):\", mean_rms)\n",
    "print(\"MFCC shape:\", mfccs.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb37081c",
   "metadata": {},
   "source": [
    "SPEECH EMOTION RECOGNITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "247b8977",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 1 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neu: 0.0628\n",
      "hap: 0.0020\n",
      "ang: 0.9352\n",
      "sad: 0.0000\n",
      "\n",
      "Predicted Emotion: ang\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForSequenceClassification, AutoFeatureExtractor\n",
    "import torch, librosa\n",
    "import torch.nn.functional as F\n",
    "\n",
    "try:\n",
    "    del model\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "extractor = AutoFeatureExtractor.from_pretrained(\"superb/wav2vec2-base-superb-er\")\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
    "    \"superb/wav2vec2-base-superb-er\", \n",
    "    trust_remote_code=True, \n",
    "    use_safetensors=True\n",
    ")\n",
    "\n",
    "# Load audio\n",
    "y, sr = librosa.load(\"output.wav\", sr=16000)\n",
    "\n",
    "# Extract features\n",
    "inputs = extractor(y, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# Convert logits ‚Üí probabilities\n",
    "probs = F.softmax(logits, dim=-1).squeeze()\n",
    "\n",
    "# Get labels\n",
    "labels = model.config.id2label\n",
    "\n",
    "# Print probabilities\n",
    "for i, p in enumerate(probs):\n",
    "    print(f\"{labels[i]}: {p.item():.4f}\")\n",
    "\n",
    "# Most likely emotion\n",
    "pred = torch.argmax(probs).item()\n",
    "print(\"\\nPredicted Emotion:\", labels[pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "55505a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)  # should be >= 2.6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7ed696",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
